{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "num_of_input_nodes = 4\n",
    "num_of_hidden_nodes = 256\n",
    "num_of_output_nodes = 3\n",
    "length_of_sequences = 24\n",
    "num_of_training_epochs = 1000\n",
    "size_of_mini_batch = 20\n",
    "num_of_prediction_epochs = 20\n",
    "learning_rate = 0.01\n",
    "forget_bias = 0.5\n",
    "num_of_sample = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, X, t, epoch):\n",
    "\n",
    "    if (batch_size + length_of_sequences + epoch) > 6164:\n",
    "        epoch = epoch - 6164 - batch_size - length_of_sequences\n",
    "    xs = np.array([[X[ i + j + epoch] for i in range(length_of_sequences)] for j in range(batch_size)])\n",
    "    ts = np.array([t[i + length_of_sequences + epoch] for i in range(batch_size)])\n",
    "\n",
    "    return xs, ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(nb_of_samples, sequence_len):\n",
    "    shift_num = -24\n",
    "    # CSVファイルから過去レートを読み込む\n",
    "    df = pd.read_csv(\"USDJPY_hourly2010_train.txt\")\n",
    "    # 最後の列にHIGHを追加\n",
    "    df_shift = df.copy()\n",
    "    df_shift['ANS'] = df['HIGH']\n",
    "    df_shift.ANS = df_shift.ANS.shift(shift_num)\n",
    "\n",
    "    # 最後の行を除外\n",
    "    df_shift = df_shift[:shift_num]\n",
    "\n",
    "    df_shift.loc[df_shift['HIGH'] == df_shift['ANS'], 'EQ'] = 1.\n",
    "    df_shift.loc[df_shift['HIGH'] != df_shift['ANS'], 'EQ'] = 0.\n",
    "    df_shift.loc[df_shift['HIGH'] < df_shift['ANS'], 'UP'] = 1.\n",
    "    df_shift.loc[df_shift['HIGH'] >= df_shift['ANS'], 'UP'] = 0.\n",
    "    df_shift.loc[df_shift['HIGH'] > df_shift['ANS'], 'DN'] = 1.\n",
    "    df_shift.loc[df_shift['HIGH'] <= df_shift['ANS'], 'DN'] = 0.\n",
    "    \n",
    "    # 念のためデータをdf_2として新しいデータフレームへコピ−\n",
    "    df_2 = df_shift.copy()\n",
    "    # time（時間）を削除\n",
    "    del df_2['DTYYYYMMDD']\n",
    "    del df_2['TIME']\n",
    "    del df_2['TICKER']\n",
    "    del df_2['ANS']\n",
    "    \n",
    "    df_3 = df_2.copy()\n",
    "    del df_3['OPEN']\n",
    "    del df_3['HIGH']\n",
    "    del df_3['LOW']\n",
    "    del df_3['CLOSE']\n",
    "\n",
    "    # データセットの行数と列数を格納\n",
    "    n = df_2.shape[0]\n",
    "    p = df_2.shape[1]\n",
    "    # 訓練データとテストデータへ切り分け\n",
    "    #train_start = 0\n",
    "    #train_end = int(np.floor(0.8*n))\n",
    "    #test_start = train_end + 1\n",
    "    #test_end = n\n",
    "    #data_train = df_2.loc[np.arange(train_start, train_end), :]\n",
    "    #data_test = df_2.loc[np.arange(test_start, test_end), :]\n",
    "\n",
    "    data_train = df_2\n",
    "    data_test = df_3\n",
    "    \n",
    "    # データの正規化\n",
    "    scaler1 = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler1.fit(data_train)\n",
    "    data_train_norm = scaler1.transform(data_train)\n",
    "\n",
    "    scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler2.fit(data_test)\n",
    "    data_test_norm = scaler2.transform(data_test)\n",
    "    \n",
    "    # 特徴量とターゲットへ切り分け\n",
    "    X_train = data_train_norm[:, 0:4]\n",
    "    y_train = data_test_norm[:,]\n",
    "\n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(nb_of_samples,epoch):\n",
    "    sequence_len = 24\n",
    "    txs, tts = create_data(nb_of_samples, sequence_len)\n",
    "    if (size_of_mini_batch + length_of_sequences + epoch) > 6164:\n",
    "        epoch = epoch - 6164 - size_of_mini_batch - length_of_sequences\n",
    "    rxs = np.array([[txs[ i + j + epoch] for i in range(length_of_sequences)] for j in range(size_of_mini_batch)])\n",
    "    rts = np.array([tts[i + length_of_sequences + epoch] for i in range(size_of_mini_batch)])\n",
    "    return rxs, rts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_ph, istate_ph):\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "        weight1_var = tf.Variable(tf.truncated_normal(\n",
    "            [num_of_input_nodes, num_of_hidden_nodes], stddev=0.1), name=\"weight1\")\n",
    "        weight2_var = tf.Variable(tf.truncated_normal(\n",
    "            [num_of_hidden_nodes, num_of_output_nodes], stddev=0.1), name=\"weight2\")\n",
    "        bias1_var = tf.Variable(tf.truncated_normal([num_of_hidden_nodes], stddev=0.1), name=\"bias1\")\n",
    "        bias2_var = tf.Variable(tf.truncated_normal([num_of_output_nodes], stddev=0.1), name=\"bias2\")\n",
    "\n",
    "        in1 = tf.transpose(input_ph, [1, 0, 2])\n",
    "        in2 = tf.reshape(in1, [-1, num_of_input_nodes])\n",
    "        in3 = tf.matmul(in2, weight1_var) + bias1_var\n",
    "        in4 = tf.split(in3, length_of_sequences, 0)\n",
    "\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_of_hidden_nodes, forget_bias=forget_bias, state_is_tuple=False)\n",
    "        rnn_output, states_op = tf.contrib.rnn.static_rnn(cell, in4, initial_state=istate_ph)\n",
    "        #output_op = tf.matmul(rnn_output[-1], weight2_var) + bias2_var\n",
    "        output_op = tf.nn.softmax(tf.matmul(rnn_output[-1], weight2_var) + bias2_var)\n",
    "\n",
    "        # Add summary ops to collect data\n",
    "        w1_hist = tf.summary.histogram(\"weights1\", weight1_var)\n",
    "        w2_hist = tf.summary.histogram(\"weights2\", weight2_var)\n",
    "        b1_hist = tf.summary.histogram(\"biases1\", bias1_var)\n",
    "        b2_hist = tf.summary.histogram(\"biases2\", bias2_var)\n",
    "        output_hist = tf.summary.histogram(\"output\",  output_op)\n",
    "        results = [weight1_var, weight2_var, bias1_var,  bias2_var]\n",
    "        return output_op, states_op, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output_op, supervisor_ph):\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        square_error = tf.reduce_mean(tf.square(output_op - supervisor_ph))\n",
    "        loss_op = square_error\n",
    "        tf.summary.scalar(\"loss\", loss_op)\n",
    "        return loss_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(loss_op):\n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        training_op = optimizer.minimize(loss_op)\n",
    "        return training_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(output_op, prints=False, epoch=0):\n",
    "    inputs, ts = make_prediction(num_of_prediction_epochs,epoch)\n",
    "    pred_dict = {\n",
    "        input_ph:  inputs,\n",
    "        supervisor_ph: ts,\n",
    "        istate_ph:    np.zeros((num_of_prediction_epochs, num_of_hidden_nodes * 2)),\n",
    "    }\n",
    "    output = sess.run([output_op], feed_dict=pred_dict)\n",
    "    good = 0\n",
    "    for i in range(0, len(ts)):\n",
    "        #print(\"=====[Output]=====[\",i,\"]\")\n",
    "        #print(output[0][i][0])\n",
    "        #print(\"=====[Output]=====[\",i,\"]\")\n",
    "        #print(\"=====[ts]=====[\",i,\"]\")\n",
    "        #print(ts[i][0])\n",
    "        #print(\"=====[ts]=====[\",i,\"]\")\n",
    "        #print(\"=====[output - ts]=====[\",i,\"]\")\n",
    "        #print(abs(output - ts)[0][i])\n",
    "        #print(\"=====[output - ts]=====[\",i,\"]\")\n",
    "        decisionOutput = 0\n",
    "        for j in range(0,3):\n",
    "            if output[0][i][decisionOutput] < output[0][i][j]:\n",
    "                decisionOutput = j\n",
    "        decisionTs = 0\n",
    "        for j in range(0,3):\n",
    "            if ts[i][decisionTs] < ts[i][j]:\n",
    "                decisionTs = j\n",
    "        if decisionOutput == decisionTs:\n",
    "            print(\"Correct: \",decisionTs,\" Output: \",decisionOutput)\n",
    "            good = good + 1\n",
    "        else :\n",
    "            print(\"Correct: \",decisionTs,\" Output: \",decisionOutput)\n",
    "    print(\"Total: \",len(ts),\" Correct: \",good)\n",
    "\n",
    "    def print_result(i, p, q):\n",
    "        [print(list(x)) for x in i]\n",
    "        print(\"output: %f, correct: %f\" % (p, q))\n",
    "    if prints:\n",
    "        [print_result(i, p, q) for i, p, q in zip(inputs, output[0], ts)]\n",
    "\n",
    "    opt = abs(output - ts)[0]\n",
    "\n",
    "    total = sum([1 if x[0] < 0.05 else 0 for x in opt])\n",
    "    #print(\"accuracy %f\" % (total / float(len(ts))))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-7-445c64bcf3d5>:15: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f87bd773a58>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-7-445c64bcf3d5>:16: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "train#0, train loss: 2.170877e-01\n",
      "Correct:  2  Output:  1\n",
      "Correct:  2  Output:  1\n",
      "Correct:  2  Output:  1\n",
      "Correct:  2  Output:  1\n",
      "Correct:  2  Output:  1\n",
      "Correct:  2  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Total:  20  Correct:  14\n",
      "train#100, train loss: 1.804042e-01\n",
      "train#200, train loss: 1.094189e-01\n",
      "train#300, train loss: 1.449090e-01\n",
      "train#400, train loss: 1.550474e-01\n",
      "train#500, train loss: 1.525713e-01\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  1  Output:  1\n",
      "Correct:  2  Output:  1\n",
      "Correct:  2  Output:  1\n",
      "Correct:  2  Output:  1\n",
      "Correct:  2  Output:  1\n",
      "Correct:  2  Output:  1\n",
      "Total:  20  Correct:  15\n",
      "train#600, train loss: 1.524232e-01\n",
      "train#700, train loss: 1.610010e-01\n",
      "train#800, train loss: 1.896310e-01\n",
      "train#900, train loss: 9.795967e-02\n",
      "Tensor(\"inference/Softmax:0\", shape=(?, 3), dtype=float32)\n",
      "Correct:  2  Output:  2\n",
      "Correct:  2  Output:  2\n",
      "Correct:  2  Output:  2\n",
      "Correct:  2  Output:  2\n",
      "Correct:  2  Output:  2\n",
      "Correct:  2  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Correct:  1  Output:  2\n",
      "Total:  20  Correct:  6\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "X, t = create_data(num_of_sample, length_of_sequences)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    input_ph = tf.placeholder(tf.float32, [None, length_of_sequences, num_of_input_nodes], name=\"input\")\n",
    "    supervisor_ph = tf.placeholder(tf.float32, [None, num_of_output_nodes], name=\"supervisor\")\n",
    "    istate_ph = tf.placeholder(tf.float32, [None, num_of_hidden_nodes * 2], name=\"istate\")\n",
    "\n",
    "    output_op, states_op, datas_op = inference(input_ph, istate_ph)\n",
    "    loss_op = loss(output_op, supervisor_ph)\n",
    "    training_op = training(loss_op)\n",
    "\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        summary_writer = tf.summary.FileWriter(\"/tmp/tensorflow_log\", graph=sess.graph)\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(num_of_training_epochs):\n",
    "            inputs, supervisors = get_batch(size_of_mini_batch, X, t, epoch)\n",
    "            train_dict = {\n",
    "                input_ph:      inputs,\n",
    "                supervisor_ph: supervisors,\n",
    "                istate_ph:     np.zeros((size_of_mini_batch, num_of_hidden_nodes * 2)),\n",
    "            }\n",
    "            sess.run(training_op, feed_dict=train_dict)\n",
    "\n",
    "            if (epoch) % 100 == 0:\n",
    "                summary_str, train_loss = sess.run([summary_op, loss_op], feed_dict=train_dict)\n",
    "                print(\"train#%d, train loss: %e\" % (epoch, train_loss))\n",
    "                summary_writer.add_summary(summary_str, epoch)\n",
    "                if (epoch) % 500 == 0:\n",
    "                    calc_accuracy(output_op, epoch=epoch)\n",
    "        print(output_op)\n",
    "        calc_accuracy(output_op, prints=False)\n",
    "        datas = sess.run(datas_op)\n",
    "        #saver.save(sess, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
